<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audial - Advanced Voice Recorder</title> <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        /* Basic font setting for consistency */
        body {
            font-family: 'Inter', sans-serif;
        }

        /* Smooth transition for the record button's visual feedback */
        .record-button {
            transition: background-color 0.2s ease-in-out, transform 0.1s ease-out; /* Added transform for a subtle click effect */
        }

        /* State for when recording is active */
        .record-button.recording {
            background-color: #d64545; /* Tailwind red-500 */
            /* Maybe a subtle pulse or shadow for extra emphasis? */
            box-shadow: 0 0 15px rgba(239, 68, 68, 0.5); /* Soft red glow */
        }

        /* Hover effect specifically for the recording state */
        .record-button.recording:hover {
            background-color: #e26843; /* Tailwind red-600 */
            transform: scale(1.02); /* Slight scale on hover while recording */
        }

        /* Style for the transcription display area */
        #transcriptionDisplay {
            min-height: 60px; /* A bit more breathing room */
            padding: 15px; 
            background-color: #f3f4f6; 
            border-radius: 12px; 
            font-size: 1.125rem;
            color: #4b5563; 
            text-align: center;
            overflow-y: auto; 
            max-height: 180px; 
            line-height: 1.6;
            border: 1px solid #e5e7eb; /* Subtle border for definition */
        }
        
        /* Loader animation for AI summary */
        .summary-loader {
            border: 4px solid #f3f3f3; /* Light gray base */
            border-top: 4px solid #6b21a8; /* A deep purple, feels a bit more custom than just '330446' */
            border-radius: 50%;
            width: 28px; /* Slightly larger loader */
            height: 28px;
            animation: spin 1s linear infinite;
            display: inline-block;
            vertical-align: middle;
            margin-left: 10px; /* More space */
        }

        /* Keyframe animation for the spinner */
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        /* Adding a bit of custom flair for the app container shadow */
        #app-container {
            box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04); /* More pronounced shadow */
        }

        /* Customizing checkbox appearance - usually done with Tailwind plugins or direct CSS */
        .form-checkbox:checked {
            background-color: #2563eb; /* A standard blue, text-blue-600 */
            border-color: #2563eb;
        }
        .form-checkbox:focus {
            box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.5); /* Focus ring */
            outline: none;
        }

    </style>
</head>
<body class="bg-gray-50 text-gray-800 flex items-center justify-center min-h-screen p-4 sm:p-6"> <div id="app-container" class="relative w-full max-w-4xl mx-auto bg-white shadow-xl rounded-xl overflow-hidden" style="min-height: 600px; max-height: 80vh;"> <div class="absolute top-4 right-4 z-10">
            <a href="more.html" class="inline-block"> <button class="bg-gray-100 hover:bg-gray-200 text-gray-700 font-semibold py-2 px-4 rounded-full shadow-sm text-sm transition-all duration-200 ease-in-out hover:scale-105">
                    Explore Other Tools
                </button>
            </a>
        </div>
        <div class="flex flex-col md:flex-row h-full"> <div class="w-full md:w-1/3 bg-gray-50 border-b md:border-b-0 md:border-r border-gray-200 p-6 flex flex-col">
                <h2 class="text-xl font-bold mb-5 text-gray-700">Your Session Recordings</h2> <div id="recordingsList" class="space-y-4 overflow-y-auto flex-grow pr-2"> </div>
            </div>

            <div class="w-full md:w-2/3 flex flex-col items-center justify-between p-8 sm:p-10"> <div class="flex items-center space-x-3 mb-8"> <input type="checkbox" id="transcriptionToggle" class="form-checkbox h-5 w-5 text-blue-500 rounded-md cursor-pointer"> <label for="transcriptionToggle" class="text-lg text-gray-700 select-none cursor-pointer">Enable Live Transcription</label>
                </div>

                <div class="flex-grow flex items-center justify-center flex-col w-full px-4"> <div id="timer" class="text-7xl lg:text-8xl font-mono font-extrabold text-gray-900 mb-6 tracking-tight">00:00</div> <div id="transcriptionDisplay" class="w-full text-center text-base text-gray-600 border border-gray-300 rounded-lg p-3 overflow-y-auto max-h-40 shadow-inner">
                        <p class="text-gray-400 italic">Start recording to see live transcription...</p> </div>
                </div>
                
                <div class="flex-shrink-0 mt-10"> <button id="recordButton" class="record-button bg-blue-500 hover:bg-blue-600 text-white font-bold py-4 px-16 rounded-full shadow-lg focus:outline-none focus:ring-4 focus:ring-blue-300 text-2xl uppercase tracking-wide"> Start Recording
                    </button>
                </div>
            </div>
        </div>
    </div>

    <div id="errorModal" class="hidden fixed inset-0 bg-black bg-opacity-60 z-50 flex items-center justify-center p-4"> <div class="bg-white p-7 rounded-lg shadow-2xl max-w-sm w-full mx-auto text-center">
            <h3 class="text-xl font-bold mb-4 text-red-600">Oops! Something Went Wrong</h3> <p id="errorMessage" class="text-gray-700 mb-5"></p>
            <button onclick="document.getElementById('errorModal').classList.add('hidden')" class="mt-4 bg-red-500 hover:bg-red-600 text-white font-bold py-3 px-6 rounded-lg w-full transition-colors duration-200">Got It</button> </div>
    </div>

    <div id="summaryModal" class="hidden fixed inset-0 bg-black bg-opacity-60 z-50 flex items-center justify-center p-4">
        <div class="bg-white p-7 rounded-lg shadow-2xl max-w-lg w-full mx-auto">
            <h3 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                Summary Generated by AI
                <span id="summaryLoader" class="summary-loader hidden"></span>
            </h3>
            <div id="summaryContent" class="text-gray-700 max-h-80 overflow-y-auto border border-gray-200 p-4 rounded-md bg-gray-50 leading-relaxed text-base"></div> <button onclick="document.getElementById('summaryModal').classList.add('hidden'); document.getElementById('summaryContent').textContent = '';" class="mt-6 bg-purple-600 hover:bg-purple-700 text-white font-bold py-3 px-6 rounded-lg w-full transition-colors duration-200">Close Summary</button> </div>
    </div>


    <script type="module">
        // Core recording variables
        let mediaRecorder;
        let audioChunks = [];
        let timerInterval;
        let seconds = 0;
        let isRecording = false;
        let sessionRecordings = []; // Renamed from localRecordings for a slightly better feel
        
        // Transcription-related variables
        let speechRecognition; // Renamed 'recognition' for clarity, avoids conflict if other 'recognition' vars were added
        let currentLiveTranscription = ''; // More descriptive name
        let isTranscriptionEnabled = false;

        // Cache DOM elements for performance and readability
        const recordButton = document.getElementById('recordButton');
        const timerDisplay = document.getElementById('timer');
        const recordingsList = document.getElementById('recordingsList');
        const errorModal = document.getElementById('errorModal');
        const errorMessage = document.getElementById('errorMessage');
        const transcriptionToggle = document.getElementById('transcriptionToggle');
        const transcriptionDisplay = document.getElementById('transcriptionDisplay');
        const summaryModal = document.getElementById('summaryModal');
        const summaryContent = document.getElementById('summaryContent');
        const summaryLoader = document.getElementById('summaryLoader');


        /**
         * Formats seconds into a human-readable MM:SS string.
         * @param {number} totalSeconds - The total number of seconds.
         * @returns {string} Formatted time string.
         */
        function formatTime(totalSeconds) {
            const minutes = Math.floor(totalSeconds / 60).toString().padStart(2, '0');
            const secs = (totalSeconds % 60).toString().padStart(2, '0'); // Renamed 'seconds' to 'secs'
            return `${minutes}:${secs}`;
        }

        /**
         * Displays a user-friendly error message in a modal.
         * @param {string} message - The error message to display.
         */
        function displayError(message) { // Renamed from showError
            errorMessage.textContent = message;
            errorModal.classList.remove('hidden');
        }
        
        /**
         * Converts an audio Blob into a Base64 Data URL.
         * Useful for storing or previewing audio in the browser.
         * @param {Blob} blob - The audio blob to convert.
         * @returns {Promise<string>} A promise that resolves with the Base64 Data URL.
         */
        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => resolve(reader.result);
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        /**
         * Triggers a download of the provided Data URL as a file.
         * @param {string} dataUrl - The Base64 Data URL of the file.
         * @param {string} filename - The desired filename for the download.
         */
        function triggerDownload(dataUrl, filename) { // Renamed from downloadRecording
            const a = document.createElement('a');
            a.href = dataUrl;
            a.download = filename;
            document.body.appendChild(a); // Append to body is crucial for some browsers
            a.click();
            document.body.removeChild(a); // Clean up
        }

        /**
         * Toggles the recording state (start/stop).
         * This function acts as the main entry point for the record button.
         */
        async function toggleRecording() {
            if (isRecording) {
                stopRecording();
            } else {
                await startRecording();
            }
        }

        /**
         * Initiates the audio recording process and sets up transcription if enabled.
         */
        async function startRecording() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                // Reset states for a new recording
                audioChunks = [];
                seconds = 0;
                currentLiveTranscription = '';
                transcriptionDisplay.textContent = 'Listening for your voice...'; // Friendlier message

                // Event handler for when audio data becomes available
                mediaRecorder.ondataavailable = event => {
                    if (event.data.size > 0) { // Only push if there's actual data
                        audioChunks.push(event.data);
                    }
                };

                // Event handler for when recording stops
                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    try {
                        const audioDataUrl = await blobToBase64(audioBlob);
                        saveRecordingToSession(audioDataUrl, currentLiveTranscription); // Renamed function
                    } catch (error) {
                        console.error("Failed to process recording:", error);
                        displayError("Couldn't save your recording. Please try again!"); // More conversational error
                    } finally {
                        // Always stop media tracks to release microphone
                        stream.getTracks().forEach(track => track.stop());
                        // Also stop speech recognition if it was running
                        if (speechRecognition && isTranscriptionEnabled) {
                            speechRecognition.stop();
                        }
                    }
                };

                mediaRecorder.start();
                isRecording = true;
                recordButton.textContent = 'Stop Recording'; // Clearer CTA
                recordButton.classList.add('recording');
                
                // Start the timer
                timerInterval = setInterval(() => {
                    seconds++;
                    timerDisplay.textContent = formatTime(seconds);
                }, 1000);

                // If transcription is enabled, kick it off
                if (isTranscriptionEnabled) {
                    startLiveTranscription(); // Renamed function
                }

            } catch (err) {
                console.error("Microphone access error:", err);
                // Provide specific feedback for common errors
                if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
                    displayError("We need microphone access to record! Please allow it in your browser settings.");
                } else if (err.name === "NotFoundError") {
                    displayError("No microphone found. Please connect one and try again.");
                } else {
                    displayError("An unexpected error occurred accessing your microphone. (" + err.message + ")");
                }
                isRecording = false; // Ensure state is correct
                if (speechRecognition) {
                    speechRecognition.stop();
                }
            }
        }

        /**
         * Stops the current audio recording.
         */
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === "recording") { // Check state before stopping
                mediaRecorder.stop();
            }
            isRecording = false;
            recordButton.textContent = 'Start Recording'; // Reset button text
            recordButton.classList.remove('recording');
            clearInterval(timerInterval); // Stop the timer
            timerDisplay.textContent = '00:00'; // Reset timer display
            if (speechRecognition && isTranscriptionEnabled) { // Ensure recognition is stopped gracefully
                speechRecognition.stop();
            }
        }

        /**
         * Initializes and starts the Web Speech API for live transcription.
         */
        function startLiveTranscription() { // Renamed from startTranscription
            // Check for browser compatibility
            window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            if (!window.SpeechRecognition) {
                displayError("Your browser doesn't support live transcription. Try Chrome or Edge!"); // More helpful message
                isTranscriptionEnabled = false; // Disable toggle
                transcriptionToggle.checked = false;
                return;
            }

            speechRecognition = new SpeechRecognition();
            speechRecognition.interimResults = true; // Show results as they come in
            speechRecognition.continuous = true; // Keep listening
            speechRecognition.lang = 'en-US'; // Specify language

            // Process transcription results
            speechRecognition.onresult = event => {
                let interimTranscript = '';
                let finalTranscript = '';

                // Loop through the results to distinguish final from interim
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript + ' ';
                    } else {
                        interimTranscript += transcript;
                    }
                }
                currentLiveTranscription = finalTranscript + interimTranscript;
                transcriptionDisplay.textContent = currentLiveTranscription || 'Still listening...'; // Dynamic message
            };

            // Handle errors from the speech recognition service
            speechRecognition.onerror = event => {
                console.error('Speech Recognition Error:', event.error);
                let message = `Transcription error: "${event.error}".`;
                if (event.error === 'not-allowed') {
                    message = "Microphone permission might be blocked for transcription. Check your browser settings."; 
                } else if (event.error === 'no-speech') {
                    message = "Didn't hear any speech. Make sure your microphone is working!";
                } else if (event.error === 'network') {
                    message = "Network issue. Please check your internet connection for transcription.";
                }
                displayError(message);
                
                // Clean up recognition if an error occurs
                if (speechRecognition) {
                    speechRecognition.stop();
                }
                // If recording was active, stop that too for a clean slate
                if (isRecording) {
                    stopRecording();
                }
            };
            
            // What to do when speech recognition ends
            speechRecognition.onend = () => {
                // If we're still recording and transcription is enabled, try restarting it
                if (isRecording && isTranscriptionEnabled) {
                    console.log("Speech recognition stopped, attempting restart...");
                    try {
                         speechRecognition.start();
                    } catch (e) {
                        console.warn("Failed to restart speech recognition:", e);
                        displayError("Couldn't restart live transcription. Please try recording again.");
                    }
                } else {
                    console.log("Speech recognition gracefully stopped.");
                    transcriptionDisplay.textContent = currentLiveTranscription || 'Transcription ended.'; // Final message
                }
            };

            // Start the recognition service
            speechRecognition.start();
            transcriptionDisplay.textContent = 'Listening for your voice...'; // Initial state
        }

        /**
         * Sends the transcription text to the Gemini API for summarization.
         * Shows a loading state and then displays the summary.
         * @param {string} textToSummarize - The transcription text to be summarized.
         */
        async function getTranscriptionSummary(textToSummarize) { // Renamed function
            summaryModal.classList.remove('hidden');
            summaryContent.textContent = 'Generating summary, please wait...'; // User-friendly loading
            summaryLoader.classList.remove('hidden');

            if (!textToSummarize || textToSummarize.trim() === '') {
                summaryContent.textContent = "There's no text to summarize for this recording. Try a longer one!";
                summaryLoader.classList.add('hidden');
                return;
            }

            try {
                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: `Please provide a concise summary of the following audio transcription. Focus on key points and main ideas:\n\n"${textToSummarize}"` }] });
                
                const payload = { contents: chatHistory };
                // IMPORTANT: Replace this with your actual, securely managed API key.
                // For a real app, this should be handled server-side, not directly in client-side code.
                const apiKey = "AIzaSyC0iXmXyUU_rMXFLCF8T63_mUDIgzCl8Io"; // Placeholder! Replace with your key.
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${apiKey}`; // Using gemini-pro for better summaries

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    const errorDetails = await response.json();
                    throw new Error(`API call failed: ${response.status} - ${errorDetails.error?.message || 'Unknown error'}`);
                }

                const result = await response.json();

                // Check for valid response from Gemini API
                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const summary = result.candidates[0].content.parts[0].text;
                    summaryContent.textContent = summary;
                } else {
                    summaryContent.textContent = "Couldn't get a summary right now. The AI might be busy.";
                    console.error("Unexpected or empty API response:", result);
                }
            } catch (error) {
                console.error("Error calling Gemini API:", error);
                summaryContent.textContent = `Sorry, couldn't generate a summary: ${error.message}. Please try again later!`;
            } finally {
                summaryLoader.classList.add('hidden'); // Hide loader regardless of success/failure
            }
        }

        /**
         * Saves a new recording's data and transcription to the session's list.
         * @param {string} audioDataUrl - The Base64 Data URL of the recorded audio.
         * @param {string} transcriptionText - The transcribed text of the recording.
         */
        function saveRecordingToSession(audioDataUrl, transcriptionText) { // Renamed from saveRecordingLocally
            const recordingId = `rec-${Date.now()}`; // More human-readable ID prefix
            const newRecording = {
                id: recordingId,
                name: `Voice Memo ${new Date().toLocaleTimeString('en-US', { hour: '2-digit', minute: '2-digit', hour12: true })}`, // Nicer name format
                timestamp: new Date().toISOString(),
                audioDataUrl: audioDataUrl,
                transcription: transcriptionText.trim() // Trim whitespace
            };
            sessionRecordings.unshift(newRecording); // Add to the beginning (most recent first)
            renderRecordingsList(sessionRecordings); // Re-render the list
        }
        

        function deleteRecordingFromSession(id) {
            if (!confirm('Are you sure you want to delete this recording? This cannot be undone.')) { // Confirmation dialog
                return;
            }
            sessionRecordings = sessionRecordings.filter(rec => rec.id !== id);
            renderRecordingsList(sessionRecordings); 
        }

        
         
        function renderRecordingsList(recordings) { 
            recordingsList.innerHTML = ''; 
            if (recordings.length === 0) {
                recordingsList.innerHTML = `<p class="text-gray-500 text-center py-4">No recordings yet! Start a new one to see it appear here.</p>`; // Friendlier empty state
                return;
            }
            recordings.forEach(rec => {
                if (!rec.audioDataUrl) return; // Skip invalid entries

                const card = document.createElement('div');
                card.className = 'bg-white p-4 rounded-lg border border-gray-200 shadow-sm transition-all duration-150 hover:shadow-md'; // Added hover effect

                const nameEl = document.createElement('p');
                nameEl.className = 'font-semibold text-gray-800 truncate text-base mb-1';
                nameEl.textContent = rec.name;

                const dateEl = document.createElement('p');
                dateEl.className = 'text-xs text-gray-500 mb-3';
                dateEl.textContent = new Date(rec.timestamp).toLocaleString(); // Use local string for date/time
                
                const audioEl = document.createElement('audio');
                audioEl.controls = true;
                audioEl.src = rec.audioDataUrl;
                audioEl.className = 'w-full my-3 rounded-md'; // Add some styling to audio player

                // Display transcription if available
                if (rec.transcription && rec.transcription !== '') { // Check for empty string too
                    const transcriptionEl = document.createElement('p');
                    transcriptionEl.className = 'text-sm text-gray-600 italic border-t border-gray-100 pt-3 mt-3 max-h-24 overflow-y-auto'; // Adjusted styling
                    transcriptionEl.textContent = `"${rec.transcription}"`;
                    card.appendChild(transcriptionEl);
                }

                const actionsDiv = document.createElement('div');
                actionsDiv.className = 'flex flex-wrap gap-2 mt-4'; // Flex wrap for smaller screens

                // Summarize button (only if transcription exists)
                if (rec.transcription && rec.transcription !== '') {
                    const summarizeBtn = document.createElement('button');
                    summarizeBtn.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-robot inline-block mr-1 -mt-0.5" viewBox="0 0 16 16">
                            <path d="M6 12.5a.5.5 0 0 1 .5-.5h3a.5.5 0 0 1 0 1h-3a.5.5 0 0 1-.5-.5M.5 12V3a.5.5 0 0 1 .5-.5h13a.5.5 0 0 1 .5.5v9a.5.5 0 0 1-.5.5h-13a.5.5 0 0 1-.5-.5M3 8.5a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5zm5-1.5a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5zm2.5-.5h-1a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5zm-5.5 4a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5zm5-1a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5zm2.5 0h-1a.5.5 0 0 0-.5.5v1a.5.5 0 0 0 .5.5h1a.5.5 0 0 0 .5-.5v-1a.5.5 0 0 0-.5-.5z"/>
                        </svg>
                        Get Summary
                    `;
                    summarizeBtn.className = 'bg-purple-500 hover:bg-purple-600 text-white font-bold py-2 px-3 rounded-md shadow-sm text-sm flex items-center justify-center transition-colors duration-200';
                    summarizeBtn.onclick = () => getTranscriptionSummary(rec.transcription);
                    actionsDiv.appendChild(summarizeBtn);
                }

                // Download button
                const downloadBtn = document.createElement('button');
                downloadBtn.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-download inline-block mr-1 -mt-0.5" viewBox="0 0 16 16">
                        <path d="M.5 9.9a.5.5 0 0 1 .5.5v2.5a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1v-2.5a.5.5 0 0 1 1 0v2.5a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2v-2.5a.5.5 0 0 1 .5-.5"/>
                        <path d="M7.646 11.854a.5.5 0 0 0 .708 0l3-3a.5.5 0 0 0-.708-.708L8.5 10.293V1.5a.5.5 0 0 0-1 0v8.793L5.354 8.146a.5.5 0 1 0-.708.708z"/>
                    </svg>
                    Download
                `;
                downloadBtn.className = 'text-blue-600 hover:text-blue-800 text-sm font-semibold py-2 px-3 rounded-md flex items-center justify-center transition-colors duration-200';
                downloadBtn.onclick = () => {
                    const filename = `Audial-Recording-${new Date().toISOString().replace(/[:.]/g, '-')}.webm`; // More robust filename
                    triggerDownload(rec.audioDataUrl, filename);
                };
                
                // Delete button
                const deleteBtn = document.createElement('button');
                deleteBtn.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-trash inline-block mr-1 -mt-0.5" viewBox="0 0 16 16">
                        <path d="M5.5 5.5A.5.5 0 0 1 6 6v6a.5.5 0 0 1-1 0V6a.5.5 0 0 1 .5-.5m2.5 0a.5.5 0 0 1 .5.5v6a.5.5 0 0 1-1 0V6a.5.5 0 0 1 .5-.5m3 .5a.5.5 0 0 0-1 0v6a.5.5 0 0 0 1 0z"/>
                        <path d="M14.5 3a1 1 0 0 1-1 1H13v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V4h-.5a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1H6a1 1 0 0 1 1-1h2a1 1 0 0 1 1 1h3.5a1 1 0 0 1 1 1zM4.118 4 4 4.059V13a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1V4.059L11.882 4zM2.5 3h11V2h-11z"/>
                    </svg>
                    Delete
                `; 
                deleteBtn.className = 'text-red-500 hover:text-red-700 py-2 px-3 rounded-md flex items-center justify-center transition-colors duration-200';
                deleteBtn.onclick = () => deleteRecordingFromSession(rec.id);
                
                actionsDiv.appendChild(downloadBtn);
                actionsDiv.appendChild(deleteBtn);
                
                card.appendChild(nameEl);
                card.appendChild(dateEl);
                card.appendChild(audioEl);
                card.appendChild(actionsDiv); // Actions div added after audio

                recordingsList.appendChild(card);
            });
        }

        /**
         * Initializes the application when the page loads.
         * Sets up event listeners and renders initial UI elements.
         */
        function initAudialApp() { // Renamed from initializeApp
            renderRecordingsList(sessionRecordings); // Display any existing recordings

            // Set up the transcription toggle listener
            transcriptionToggle.addEventListener('change', () => {
                isTranscriptionEnabled = transcriptionToggle.checked;
                if (isTranscriptionEnabled && !isRecording) {
                    transcriptionDisplay.textContent = 'Live transcription will begin when you hit "Start Recording"!';
                } else if (!isTranscriptionEnabled && !isRecording) {
                     transcriptionDisplay.textContent = 'Transcription will appear here if enabled during recording.';
                }
                // If recording is active and toggle changes, we might need to stop/restart recognition
                if (isRecording) {
                    if (isTranscriptionEnabled && (!speechRecognition || speechRecognition.state === 'inactive')) {
                        startLiveTranscription();
                    } else if (!isTranscriptionEnabled && speechRecognition && speechRecognition.state === 'recording') {
                        speechRecognition.stop();
                    }
                }
            });
            // Set initial state of transcription based on checkbox
            isTranscriptionEnabled = transcriptionToggle.checked;
        }

        // Event listener for the main record button
        recordButton.addEventListener('click', toggleRecording);
        
        // Initialize the app once the DOM is fully loaded
        window.addEventListener('load', initAudialApp); // Use DOMContentLoaded for earlier execution
        // Or for simpler apps like this, window.onload works fine but is generally fired later
    </script>
</body>
</html>